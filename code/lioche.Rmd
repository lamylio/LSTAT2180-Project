---
title: "Resampling"
author: "Lamy Lionel"
date: "06/01/2021"
output: html_document
---

```{r options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r setup}
set.seed(1)
opt.digits = 4

n = 30 
p = 0.5
M = 50000

alpha = 0.05
theta = pgeom(2, 0.5, lower.tail = F) # 0.125


# J'ai ajouté une correction de biais à l'estimateur de p, et à T.param
# Ca semble donner des résultats légérement mieux donc je l'active par défaut.

p.estim = function(obs){1/(mean(obs)+1)}
p.estim.corrected = function(obs){ph = p.estim(obs); ph-(ph*(1-ph))/length(obs)}

T.param = function(obs, corrected=T){return (round((1 - ifelse(corrected, p.estim.corrected(obs), p.estim(obs)) )^3, 4))}
T.nonparam = function(obs){return (round(sum(obs>=3)/length(obs), 4))}

sample = rgeom(n=30, prob=0.5)
sample.distribution = prop.table(table(sample))

sample.mu = mean(sample)
sample.p = p.estim.corrected(sample)

sample.T.param = T.param(sample)
sample.T.nonparam = T.nonparam(sample)

# ---
export.list = c("opt.digits", "sample", "n", "p", "M", "alpha", "theta", "sample.mu", "sample.p", "p.estim","p.estim.corrected", "T.param", "T.nonparam", "sample.T.param", "sample.T.nonparam", "T.boot")

```

```{r bootstrap helper}
T.boot = function(T.function, this.n=n, this.sample=NULL, this.M = M){
  force(this.n); force(this.sample)
  if (!is.null(this.sample)){
    sapply(1:this.M, function (i) T.function(sample(this.sample, this.n, T)))
  }else{
    sapply(1:this.M, function (i) T.function(rgeom(this.n, p)))
  }
}
```


# Parallelization

```{r parallel}
library(parallel)
cl <<- makeCluster(min(4, detectCores()))
clusterExport(cl, export.list)
```

# Batch of boostrapped samples (parallelized)

```{r boot}
T.nonparam.boot = unlist(clusterCall(cl, T.boot, T.nonparam, this.sample=sample))
T.nonparam.geom = unlist(clusterCall(cl, T.boot, T.nonparam, this.n=n))
T.param.boot = unlist(clusterCall(cl, T.boot, T.param, this.n = n))
```

# Distribution and ECDF plots
```{r plots }
x.seq = seq(0, max(sample), 1)
(function(){
  plot(x.seq, sample.distribution, pch=16, xlab = "", ylab = "")
  lines(x.seq, sample.distribution, lwd=2)
  points(x.seq, dgeom(x.seq, p), col="blue", pch=16)
  lines(x.seq, dgeom(x.seq, p), col="blue", lwd=2)
  axis(2, at = seq(-0.0, 1, 0.1))
  title(main="Distribution of the sample vs real geometric", xlab = "Observed value", ylab="Proportion")
})()

(function (){
  plot(ecdf(sample), verticals = T, pch = 16, lwd=2, xlim=c(0, 4), 
       main="Empirical CDF vs real CDF", xlab="Observed value", ylab="Cumulative prop.")
  plot(ecdf(rgeom(M, p)), verticals = T, lwd=2, pch = 16, col="blue", add=T)
})()
```



# Bias and variance

```{r bias and variance}
# I also checked with differents M
bias.nonparam = round(mean(T.nonparam.boot - theta), opt.digits)
bias.param = round(mean(T.param.boot - theta), opt.digits)

# We can simply use built-in function var(x) but.. 
variance = function(this.boot.T) round(mean(this.boot.T^2 - mean(this.boot.T)^2), opt.digits)
variance.nonparam = variance(T.nonparam.boot)
variance.param = variance(T.param.boot)
```

# Check for normality

```{r normality check}
# Graphically with barplots
# Works also with plot(density())
# or simply hist()

barplot(table(T.nonparam.boot), main=bquote("Barplot of" ~ T[n](chi)), ylab = "Frequency", xlab="Observed value")

# Now in a QQPlot

qqnorm(T.nonparam.boot, main=bquote("QQ Plot of" ~ T[n](chi))); qqline(T.nonparam.boot, col=2)

# Shapiro
shapiro.test(T.nonparam.boot[1:5000])
shapiro.test(T.nonparam.boot[100001:105000])

```

# Give a confidence interval

## Asymptotic method

```{r asymptotic CI}
CI.asym.param = function(this.mu=sample.mu, this.n=n){
  
  Q1 = (2*this.n*this.mu + 2*this.n - qchisq(1-alpha, 1)) / (2*this.n*(this.mu + 1)^2)
  Q2 = 0.5 * sqrt((4*this.n*this.mu^2 * qchisq(1-alpha, 1) + 4*this.n*this.mu * qchisq(1-alpha, 1)^2) / (n^2*(this.mu + 1)^4))
  
  low = (1-(Q1+Q2))^3
  upp = (1-(Q1-Q2))^3
  
  round(data.frame(low, upp), opt.digits)
}

CI.asym.nonparam = function(this.sample.T = sample.T.nonparam, this.variance = variance.nonparam){
  upp = this.sample.T + qnorm(1-alpha/2) * sqrt(this.variance)
  low = this.sample.T - qnorm(1-alpha/2) * sqrt(this.variance)
  round(data.frame(low, upp), opt.digits)
}

CI.asym.param.value = CI.asym.param()
CI.asym.nonparam.value = CI.asym.nonparam()
```

## Percentile boostrap method

```{r percentile CI}
CI.percent = function(this.boot.T) round(data.frame(low=quantile(this.boot.T, alpha/2), upp=quantile(this.boot.T, 1-alpha/2)), opt.digits)

CI.percent.param = CI.percent(T.param.boot)
CI.percent.nonparam = CI.percent(T.nonparam.boot)
```

## Basic boostrap method

```{r basic CI}
CI.basic = function(this.boot.T, this.sample.T) round(data.frame(low=2*this.sample.T - CI.percent(this.boot.T)$upp, upp= 2*this.sample.T - CI.percent(this.boot.T)$low), opt.digits)

CI.basic.param = CI.basic(T.param.boot, sample.T.param)
CI.basic.nonparam = CI.basic(T.nonparam.boot, sample.T.nonparam)
```

## Student-boostrap method

```{r student CI}
CI.student = function(this.boot.T, this.sample.T, this.sample = sample, this.n = n) round(data.frame(
  low=this.sample.T - sd(this.sample) * quantile(this.boot.T, 1-alpha/2) /sqrt(this.n), 
  upp=this.sample.T - sd(this.sample) * quantile(this.boot.T, alpha/2) / sqrt(this.n)
), opt.digits)

CI.student.param = CI.student(T.param.boot, sample.T.param)
CI.student.nonparam = CI.student(T.nonparam.boot, sample.T.nonparam)
```

## Iterated-T boostrap method

```{r iterated CI}
# Est-ce que ça reviendrait pas au même que de directement sampler dans T.x.boot ? 

# 15 secondes
CI.iterated = function(T.function, sampleFunction, M=5000, M2 = 500){
  
  force(sampleFunction); clusterExport(cl, c("sampleFunction", "M2"), envir = environment())
  iterated = parSapply(cl, 1:M, function(i){
  
    boot = sampleFunction
    TStar = T.boot(T.function, this.sample=boot, this.M=M2)

    # TStar = sapply(seq(1,M2), function(j){
    #   bootSample = sample(boot, n, T)
    #   T.param(bootSample)
    # })
    
    TStar.SE = sd(TStar)/sqrt(M2)
    
    (T.function(boot) - mean(TStar))/TStar.SE
  })
  
  iterated.SE = sd(iterated)/sqrt(M)
  round(data.frame(low=T.function(sample) - quantile(iterated, 1-alpha/2, na.rm = T) * iterated.SE,
             upp=T.function(sample) - quantile(iterated, alpha/2, na.rm=T) * iterated.SE, row.names = NULL), opt.digits)
  
}

# Je ne sais pas trop si je dois utiliser rgeom, sample, T.param ou T.nonparam, mais on peut choisir du coup.
CI.iterated.param = CI.iterated(T.param, sampleFunction = rgeom(n, p))
CI.iterated.nonparam = CI.iterated(T.nonparam, sampleFunction = sample(sample, n, T))


```

# Hypothesis test

## Simple method

```{r satisfied}
#TODO

# H0: thetha = 0.125

# True distribution is : T.param.boot

HT.theta.equal = function(this.theta, T.function=T.nonparam){
  
  bootSample = T.boot(T.function, this.sample=sample)
  check = abs(mean(bootSample - this.theta)) 
  #check = quantile(bootSample, alpha/2) <= this.theta & this.theta <= quantile(bootSample, 1-alpha/2)
  ifelse(check <= alpha/2, "H0: Non-rejected", "H0: Rejected")
}

HT.theta.quantile = function(this.sample){
  
  bootSample = T.boot(T.nonparam, this.sample=this.sample)
  
  check = sample.T.nonparam >= quantile(bootSample, alpha/2) & sample.T.nonparam <= quantile(bootSample, 1-alpha/2)
  ifelse(check, "H0: Non-rejected", "H0: Rejected")
}

HT.satisfied = HT.theta.equal(0.125)
HT.rejected = HT.theta.equal(0.25)
```

# Coverage of confidence interval

```{r coverage}

m = c(10,20,50,100,500)

# Attention: c'est "exponentiel", ça prend déjà ~30s
M = 200 # Monte-Carlo
B = 1000  # Iterations

# On ne fait pas l'iterated dans le coverage

cover = function(confidence) (confidence$low <= theta & theta <= confidence$upp)

clusterExport(cl, c("M", "B", "cover", "CI.asym.param", "CI.asym.nonparam", "CI.basic", "CI.percent", "CI.student"))

coverage.and.distance = (function(){
  coverage = data.frame(); distance = data.frame()
  for (n_i in m){
    
    force(n_i); clusterExport(cl, "n_i", envir = environment())
    coverage.loc = parLapply(cl, 1:B, function(i){
      
      # Comme au devoir: on re-génère B échantillons réels
      newSample = rgeom(n_i, p)
      newSample.mu = mean(newSample)
      
      # Par contre je ne suis pas certain de quel estimateur je dois utiliser ? 
      newSample.T.param = T.param(newSample)
      newSample.T.nonparam = T.nonparam(newSample)
      
      # this.sample = on utilise sample() au lieu de rgeom()
      # Devrait-on aussi passer par rgeom??
      newSample.boot.T.nonparam = T.boot(T.nonparam, this.sample = newSample)
      newSample.boot.T.param = T.boot(T.param, this.sample = newSample)
      
     # ---
      
      all = list(
        CI.asym.param(newSample.mu, n_i),
        CI.asym.nonparam(newSample.T.nonparam, var(newSample.boot.T.nonparam)),
        CI.basic(newSample.boot.T.param, newSample.T.param),
        CI.basic(newSample.boot.T.nonparam, newSample.T.nonparam),
        CI.percent(newSample.boot.T.param),
        CI.percent(newSample.boot.T.nonparam),
        CI.student(newSample.boot.T.param, newSample.T.param, newSample, n_i),
        CI.student(newSample.boot.T.nonparam, newSample.T.nonparam, newSample, n_i)
      )
      
      list(sapply(all, cover), sapply(all, function(ci) abs(ci$upp-ci$low)))
    })
    coverage = rbind(coverage, rowMeans(sapply(1:B, function(i) unlist(coverage.loc[[i]][1])))*100)
    distance = rbind(distance, rowMeans(sapply(1:B, function(i) unlist(coverage.loc[[i]][2]))))
  }
  colnames(coverage) = c("Asymp param", "Asym np", "Basic param", "Basic np", "Percent param", "Percent np", "Student param", "Student np")
  colnames(distance) = c("Asymp param", "Asym np", "Basic param", "Basic np", "Percent param", "Percent np", "Student param", "Student np")
  
  rownames(coverage) = m; rownames(distance) = m
  return(list(coverage=coverage, distance=distance))
})()

coverage = coverage.and.distance$coverage
distance = coverage.and.distance$distance
rm(coverage.and.distance)
# ---
View(coverage)
View(distance)

# Time comparison:
# utilisateur     système      écoulé 
#
# with parLapply (parallel)
#       0.17        0.24       31.06s
#
# with lapply (base)
#       63.08       1.72       76.64s 
```

```{r end}
# Close clusters
stopCluster(cl)
rm(cl)
```

